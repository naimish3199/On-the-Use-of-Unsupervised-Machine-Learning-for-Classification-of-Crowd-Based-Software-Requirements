{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics.pairwise import cosine_similarity,cosine_distances\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from src.mrr_and_ndcg import *\n",
    "from src.preprocessing import *\n",
    "from src.pretrained_embedding_models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrr_and_ndcg = MRR_NDCG()\n",
    "preprocess = Preprocessing()\n",
    "pretrained_ver = Pretrained()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data/requirements.csv\")\n",
    "data['requirements'] = data['feature']+ \", \" + data['benefit'] + '.'\n",
    "d = pd.DataFrame(list(zip(data['requirements'], data['application_domain' ])),columns = ['requirements','class'])\n",
    "d['n_class'] = d['class']\n",
    "# replacing values\n",
    "d['n_class'].replace(['Health', 'Energy', 'Entertainment', 'Safety', 'Other'],[0,1,2,3,4], inplace=True)\n",
    "labels = d['n_class']\n",
    "namelabels = data['application_domain']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pre Processing\n",
    "corpus,corp,allvocab,freq,wavg = preprocess.preprocessing(d['requirements'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2966/2966 [01:08<00:00, 43.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Bag of Words(1,2-gram) \n",
      "MRR: 0.687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2966/2966 [01:08<00:00, 43.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Bag of Words(1,2-gram) \n",
      "NDCG: 0.769\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "####### Bag of Words(1,2-gram) #######\n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "vect = CountVectorizer(binary = False, ngram_range = (1,2))\n",
    "bag_of_words = vect.fit_transform(corp)\n",
    "doc_term_matrix = bag_of_words.todense()\n",
    "print(f\"Model: Bag of Words(1,2-gram) \\nMRR: {mrr_and_ndcg.MRR(doc_term_matrix ,labels)}\")\n",
    "print(f\"Model: Bag of Words(1,2-gram) \\nNDCG: {mrr_and_ndcg.NDCG(doc_term_matrix ,labels)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######  TF-IDF(1,2-gram) #######\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "v = TfidfVectorizer(binary = False, ngram_range = (1,2))\n",
    "tf_idf = v.fit_transform(corp).todense()\n",
    "tfidfarray = v.fit_transform(corp).toarray()\n",
    "tflist = list(v.get_feature_names_out())\n",
    "print(f\"Model: TF-IDF(1,2-gram) \\nMRR: {mrr_and_ndcg.MRR(tf_idf,labels)}\")\n",
    "print(f\"Model: TF-IDF(1,2-gram) \\nNDCG: {mrr_and_ndcg.NDCG(tf_idf,labels)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2966/2966 [01:04<00:00, 46.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Doc2Vec \n",
      "MRR: 0.672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2966/2966 [01:16<00:00, 38.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Doc2Vec \n",
      "NDCG: 0.767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "####### Doc2Vec #######\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "tagged_data = [TaggedDocument(d, [i]) for i, d in enumerate(corpus)] # Convert tokenized document into gensim formated tagged data\n",
    "dmodel = Doc2Vec(tagged_data, vector_size=100, window=5, min_count=1,epochs=30)\n",
    "doc2vec_embedd = [dmodel.dv[x] for x in range(0,2966)]\n",
    "print(f\"Model: Doc2Vec \\nMRR: {mrr_and_ndcg.MRR(doc2vec_embedd,labels)}\")\n",
    "print(f\"Model: Doc2Vec \\nNDCG: {mrr_and_ndcg.NDCG(doc2vec_embedd,labels)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2966/2966 [01:39<00:00, 29.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Word2Vec(Self Trained) \n",
      "MRR: 0.726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2966/2966 [01:19<00:00, 37.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Word2Vec(Self Trained) \n",
      "NDCG: 0.769\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "####### Word2Vec(Self Trained) #######\n",
    "from gensim.models import Word2Vec\n",
    "word2vec = Word2Vec(corpus, min_count = 1,vector_size = 100,window = 5,sg = 1,epochs=30, seed = 1) # 1-> skipgram, 0-> cbow\n",
    "avgword2vec = []\n",
    "for x in corpus:\n",
    "    avgword2vec.append(np.mean([word2vec.wv[token] for token in x if token in word2vec.wv.index_to_key],axis=0))\n",
    "print(f\"Model: Word2Vec(Self Trained) \\nMRR: {mrr_and_ndcg.MRR(avgword2vec,labels)}\")\n",
    "print(f\"Model: Word2Vec(Self Trained) \\nNDCG: {mrr_and_ndcg.NDCG(avgword2vec,labels)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2966/2966 [01:03<00:00, 46.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Tf-IDF Word2Vec(Self Trained) \n",
      "MRR: 0.715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2966/2966 [01:14<00:00, 39.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Tf-IDF Word2Vec(Self Trained) \n",
      "NDCG: 0.769\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "####### Tf-IDF Word2Vec(Self Trained) ####### \n",
    "tfidfword2vec = []\n",
    "for x in range(len(corpus)):\n",
    "    z1 = [r for r in corpus[x] if len(r) > 1]\n",
    "    tfidfword2vec.append(np.mean([word2vec.wv[token]*tfidfarray[x][tflist.index(token)] for token in z1 if token in word2vec.wv.index_to_key],axis=0))\n",
    "print(f\"Model: Tf-IDF Word2Vec(Self Trained) \\nMRR: {mrr_and_ndcg.MRR(tfidfword2vec,labels)}\")\n",
    "print(f\"Model: Tf-IDF Word2Vec(Self Trained) \\nNDCG: {mrr_and_ndcg.NDCG(tfidfword2vec,labels)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2966/2966 [01:10<00:00, 41.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Word2Vec(Pre Trained) \n",
      "MRR: 0.722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2966/2966 [01:11<00:00, 41.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Word2Vec(Pre Trained) \n",
      "NDCG: 0.77\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2966/2966 [01:04<00:00, 46.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Tf-IDF Word2Vec(Pre Trained)  \n",
      "MRR: 0.712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2966/2966 [01:16<00:00, 38.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Tf-IDF Word2Vec(Pre Trained)  \n",
      "NDCG: 0.77\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#######  Word2Vec(Pre Trained) & Tf-IDF Word2Vec(Pre Trained) ####### \n",
    "from gensim.models import KeyedVectors\n",
    "import gensim.downloader as api\n",
    "w = api.load('word2vec-google-news-300')\n",
    "\n",
    "word2vec_pretrained = pretrained_ver.avg_pretrained_embedding(w, corpus)  \n",
    "word2vec_tfidf = pretrained_ver.tfidf_embedding(w,corpus,tfidfarray,tflist)\n",
    "\n",
    "print(f\"Model: Word2Vec(Pre Trained) \\nMRR: {mrr_and_ndcg.MRR(word2vec_pretrained,labels)}\")\n",
    "print(f\"Model: Word2Vec(Pre Trained) \\nNDCG: {mrr_and_ndcg.NDCG(word2vec_pretrained,labels)}\")\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "print(f\"Model: Tf-IDF Word2Vec(Pre Trained)  \\nMRR: {mrr_and_ndcg.MRR(word2vec_tfidf,labels)}\")\n",
    "print(f\"Model: Tf-IDF Word2Vec(Pre Trained)  \\nNDCG: {mrr_and_ndcg.NDCG(word2vec_tfidf,labels)}\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2966/2966 [00:57<00:00, 51.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: FastText(Self Trained)  \n",
      "MRR: 0.672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2966/2966 [01:17<00:00, 38.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: FastText(Self Trained)  \n",
      "NDCG: 0.767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#######  FastText(Self Trained) ####### \n",
    "from gensim.models import FastText\n",
    "fasttext_model = FastText(corpus, min_count=1,vector_size = 100,window = 5,epochs=30)\n",
    "avgfasttext = []\n",
    "for x in corpus:\n",
    "    avgfasttext.append(np.mean([fasttext_model.wv[token] for token in x ],axis=0))\n",
    "print(f\"Model: FastText(Self Trained)  \\nMRR: {mrr_and_ndcg.MRR(avgfasttext,labels)}\")\n",
    "print(f\"Model: FastText(Self Trained)  \\nNDCG: {mrr_and_ndcg.NDCG(avgfasttext,labels)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2966/2966 [01:06<00:00, 44.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: TF-IDF Fasttext(Self Trained)  \n",
      "MRR: 0.682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2966/2966 [01:09<00:00, 42.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: TF-IDF Fasttext(Self Trained)  \n",
      "NDCG: 0.767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "####### TF-IDF Fasttext(Self Trained) ####### \n",
    "tfidf_fasttext = []\n",
    "for x in range(len(corpus)):\n",
    "    z1 = [r for r in corpus[x] if len(r) > 1]\n",
    "    tfidf_fasttext.append(np.mean([fasttext_model.wv[token]*tfidfarray[x][tflist.index(token)] for token in z1],axis=0))\n",
    "print(f\"Model: TF-IDF Fasttext(Self Trained)  \\nMRR: {mrr_and_ndcg.MRR(tfidf_fasttext,labels)}\") \n",
    "print(f\"Model: TF-IDF Fasttext(Self Trained)  \\nNDCG: {mrr_and_ndcg.NDCG(tfidf_fasttext,labels)}\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "170\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2966/2966 [01:02<00:00, 47.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: FastText(Pre Trained)  \n",
      "MRR: 0.704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2966/2966 [01:07<00:00, 43.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: FastText(Pre Trained)  \n",
      "NDCG: 0.769\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2966/2966 [01:08<00:00, 43.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: TF-IDF Fasttext(Pre Trained)  \n",
      "MRR: 0.696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2966/2966 [01:12<00:00, 40.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: TF-IDF Fasttext(Pre Trained) \n",
      "NDCG: 0.769\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#######  FastText(Pre Trained) & TF-IDF Fasttext(Pre Trained) ####### \n",
    "modelfasttext = api.load(\"fasttext-wiki-news-subwords-300\")\n",
    "fasttext_pretrained =  pretrained_ver.avg_pretrained_embedding(modelfasttext, corpus)\n",
    "tfidf_ft = pretrained_ver.tfidf_embedding(modelfasttext,corpus,tfidfarray,tflist)\n",
    "\n",
    "print(f\"Model: FastText(Pre Trained)  \\nMRR: {mrr_and_ndcg.MRR(fasttext_pretrained,labels)}\")\n",
    "print(f\"Model: FastText(Pre Trained)  \\nNDCG: {mrr_and_ndcg.NDCG(fasttext_pretrained,labels)}\")\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "print(f\"Model: TF-IDF Fasttext(Pre Trained)  \\nMRR: {mrr_and_ndcg.MRR(tfidf_ft,labels)}\")\n",
    "print(f\"Model: TF-IDF Fasttext(Pre Trained) \\nNDCG: {mrr_and_ndcg.NDCG(tfidf_ft,labels)}\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2966/2966 [01:05<00:00, 44.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Glove(Pre Trained) \n",
      "MRR: 0.708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2966/2966 [01:08<00:00, 43.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Glove(Pre Trained) \n",
      "MRR: 0.769\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2966/2966 [01:06<00:00, 44.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: TF-IDF Glove(Pre Trained)  \n",
      "MRR: 0.697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2966/2966 [01:06<00:00, 44.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: TF-IDF Glove(Pre Trained)  \n",
      "NDCG: 0.769\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "###### Glove(Pre Trained) & TF-IDF Glove(Pre Trained) #######\n",
    "glove = api.load('glove-wiki-gigaword-300')\n",
    "glove_pretrained = pretrained_ver.avg_pretrained_embedding(glove, corpus)\n",
    "wavg_glove = pretrained_ver.weighted_avg_pretrained_embedding(glove, corpus,wavg,freq)\n",
    "tfidfglove = pretrained_ver.tfidf_embedding(glove,corpus,tfidfarray,tflist)\n",
    "print(f\"Model: Glove(Pre Trained) \\nMRR: {mrr_and_ndcg.MRR(glove_pretrained,labels)}\")\n",
    "print(f\"Model: Glove(Pre Trained) \\nMRR: {mrr_and_ndcg.NDCG(glove_pretrained,labels)}\")  \n",
    "\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "print(f\"Model: TF-IDF Glove(Pre Trained)  \\nMRR: {mrr_and_ndcg.MRR(tfidfglove,labels)}\")\n",
    "print(f\"Model: TF-IDF Glove(Pre Trained)  \\nNDCG: {mrr_and_ndcg.NDCG(tfidfglove,labels)}\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install glove-python-binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ###### Glove(Self Trained) ######\n",
    "# import glove\n",
    "# from glove import Corpus, Glove # creating a corpus object\n",
    "# corpusa = Corpus()\n",
    "# corpusa.fit(corpus,window = 5)\n",
    "# glove = Glove(100,0.05)\n",
    "# glove.fit(corpusa.matrix, epochs = 50,no_threads =4)\n",
    "# glove.add_dictionary(corpusa.dictionary)\n",
    "\n",
    "# # Glove(Self Trained)\n",
    "# avgglove = []\n",
    "# for x in corpus:\n",
    "#     avgglove.append(np.mean([glove.word_vectors[glove.dictionary[token]] for token in x ],axis=0))\n",
    "# print(f\"Model: Glove(Self Trained)  \\nMRR: {mrr_and_ndcg.MRR(avgglove,labels)}\")\n",
    "# print(f\"Model: Glove(Self Trained)  \\nNDCG: {mrr_and_ndcg.NDCG(avgglove,labels)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2966/2966 [01:19<00:00, 37.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: LaBSE \n",
      "MRR: 0.72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2966/2966 [01:40<00:00, 29.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: LaBSE \n",
      "NDCG: 0.77\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "##### LaBSE #####\n",
    "labse = SentenceTransformer('sentence-transformers/LaBSE')\n",
    "labse_embeddings = labse.encode(corp)\n",
    "print(f\"Model: LaBSE \\nMRR: {mrr_and_ndcg.MRR(labse_embeddings,labels)}\")\n",
    "print(f\"Model: LaBSE \\nNDCG: {mrr_and_ndcg.NDCG(labse_embeddings,labels)}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\naimi\\anaconda3\\envs\\assignment\\lib\\site-packages\\tensorflow_hub\\module_v2.py:126: The name tf.saved_model.load_v2 is deprecated. Please use tf.compat.v2.saved_model.load instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\naimi\\anaconda3\\envs\\assignment\\lib\\site-packages\\tensorflow_hub\\module_v2.py:126: The name tf.saved_model.load_v2 is deprecated. Please use tf.compat.v2.saved_model.load instead.\n",
      "\n",
      "100%|██████████| 2966/2966 [01:44<00:00, 28.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: USE \n",
      "MRR: 0.722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2966/2966 [01:28<00:00, 33.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: USE \n",
      "NDCG: 0.771\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "####### Universal Sentence Encoder (USE) #######\n",
    "import tensorflow_hub as hub\n",
    "url = \"https://tfhub.dev/google/universal-sentence-encoder/4\"\n",
    "use = hub.load(url)\n",
    "usi_embedd = use(corp)\n",
    "print(f\"Model: USE \\nMRR: {mrr_and_ndcg.MRR(usi_embedd,labels)}\")\n",
    "print(f\"Model: USE \\nNDCG: {mrr_and_ndcg.NDCG(usi_embedd,labels)}\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2966/2966 [01:16<00:00, 38.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: SBERT \n",
      "MRR: 0.731\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2966/2966 [01:43<00:00, 28.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: SBERT \n",
      "NDCG: 0.77\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "####### SBERT #######\n",
    "sbert = SentenceTransformer('sentence-transformers/paraphrase-MiniLM-L6-v2')\n",
    "s_embeddings = sbert.encode(corp)\n",
    "print(f\"Model: SBERT \\nMRR: {mrr_and_ndcg.MRR(s_embeddings,labels)}\")\n",
    "print(f\"Model: SBERT \\nNDCG: {mrr_and_ndcg.NDCG(s_embeddings,labels)}\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2966/2966 [01:08<00:00, 43.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: S-RoBERTa \n",
      "MRR: 0.738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2966/2966 [01:28<00:00, 33.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: S-RoBERTa \n",
      "NDCG: 0.771\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# S-RoBERTa\n",
    "sroberta = SentenceTransformer('sentence-transformers/all-distilroberta-v1')\n",
    "sroberta_emb = sroberta.encode(corp)\n",
    "print(f\"Model: S-RoBERTa \\nMRR: {mrr_and_ndcg.MRR(sroberta_emb,labels)}\")\n",
    "print(f\"Model: S-RoBERTa \\nNDCG: {mrr_and_ndcg.NDCG(sroberta_emb,labels)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "assignment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
